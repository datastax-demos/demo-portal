#!/usr/bin/env python

import socket

from optparse import OptionParser, OptionGroup
import subprocess
import shlex
from collections import namedtuple
import logging


def setup_globals():
    global logger, options, host_ip, \
        dse_ports, opscenter_ports, weather_ports

    logger = logging.getLogger(__file__)
    logger.setLevel(logging.DEBUG)
    fh = logging.FileHandler('%s.log' % __file__)
    fh.setLevel(logging.DEBUG)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    formatter = logging.Formatter(
        '[%(asctime)s] %(levelname)s %(message)s')
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    logger.addHandler(fh)
    logger.addHandler(ch)

    parser = OptionParser()

    group = OptionGroup(parser, 'Node Type Options',
                        'Set the DSE node type.')
    group.add_option('-k', '--spark',
                     action='store_true', default=False,
                     help='enable the Spark service')
    group.add_option('-s', '--solr',
                     action='store_true', default=False,
                     help='enable the Solr service')
    group.add_option('-t', '--hadoop',
                     action='store_true', default=False,
                     help='enable the Hadoop (Task Tracker) service')
    parser.add_option_group(group)

    group = OptionGroup(parser, 'Docker Options',
                        'Setup the docker environment.')
    group.add_option('--localbuilds',
                     action='store_true', default=False,
                     help='use local docker builds')
    group.add_option('--debug',
                     action='store_true', default=False,
                     help='print debug information')
    parser.add_option_group(group)

    (options, args) = parser.parse_args()

    host_ip = socket.gethostbyname(socket.gethostname())
    dse_ports = [
        # ## Public Facing Ports

        4040,  # Spark application web site port
        7080,  # Spark Master web site port
        7081,  # Spark Worker web site port
        8012,  # Hadoop Job Tracker client port
        8983,  # Solr port and Demo applications web site port
        50030,  # Hadoop Job Tracker web site port
        50060,  # Hadoop Task Tracker web site port

        # ## Inter-node Ports
        # ### Cassandra

        7000,  # Cassandra inter-node cluster communication port
        7001,  # Cassandra SSL inter-node cluster communication port
        7199,  # Cassandra JMX monitoring port
        9042,  # CQL native clients port
        9160,  # Cassandra client port (Thrift) port

        # ### DSE

        7077,  # Spark Master inter-node communication port
        8984,  # Solr inter-node communication port
        9290,  # Hadoop Job Tracker Thrift port
        10000,  # Hive/Shark server port

        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        8020,   # TODO: REMOVE: debugging weather sensors
    ]
    opscenter_ports = [
        # ## Public Facing Ports

        8888,  # OpsCenter web site port

        # ## Inter-node Ports
        # ### OpsCenter

        50031,  # OpsCenter HTTP proxy for Job Tracker port
        61620,  # OpsCenter monitoring port
        61621,  # OpsCenter agent port
    ]
    weather_ports = [
        3000,  # Application port
    ]


def run(command):
    logger.info('Running: %s' % command)
    Response = namedtuple('Response', 'command stdout stderr')
    process = subprocess.Popen(shlex.split(command),
                               stdout=subprocess.PIPE,
                               stderr=subprocess.PIPE)
    read = process.communicate()
    return Response(command,
                    read[0].strip(),
                    read[1].strip())


def stop_all():
    for line in run('docker ps').stdout.split('\n'):
        container_id = line.split()[0]
        run('docker kill %s' % container_id)


def launch_dse(debug=False, localbuilds=False,
               spark=False, solr=False, hadoop=False,
               sharkserver=False, hiveserver=False):
    launch_command = []
    launch_command.append('docker run')

    if debug:
        launch_command.append('-i')
        launch_command.append('-t')
        launch_command.append('--entrypoint /bin/bash')
    else:
        launch_command.append('-d')

    launch_command.append('-v /mnt/cassandra:/var/lib/cassandra')

    for port in dse_ports:
        launch_command.append('-p {0}:{0}'.format(port))

    launch_command.append('--name datastax-enterprise')

    if localbuilds:
        launch_command.append('datastax-enterprise')
    else:
        launch_command.append('datastaxdemos/datastax-enterprise')

    if not debug:
        if spark:
            launch_command.append('-k')
        if solr:
            launch_command.append('-s')
        if hadoop:
            launch_command.append('-t')

        if sharkserver:
            launch_command.append('--sharkserver 5588')
        if hiveserver:
            launch_command.append('--hiveserver 5587')

    launch_command = ' '.join(launch_command)

    if debug:
        print 'Manually run:'
        print launch_command
    else:
        response = run(launch_command)

        if response.stderr:
            logger.error(response)
        else:
            Response = namedtuple('Response', 'container_id ip')
            container_id = response.stdout
            ip = run('docker exec %s hostname -i' % container_id).stdout
            return Response(container_id, ip)


def launch_opscenter(debug=False, localbuilds=False, cluster_ip=False, link=False):
    launch_command = []
    launch_command.append('docker run')

    if debug:
        launch_command.append('-i')
        launch_command.append('-t')
        launch_command.append('--entrypoint /bin/bash')
    else:
        launch_command.append('-d')

    for port in opscenter_ports:
        launch_command.append('-p {0}:{0}'.format(port))

    launch_command.append('--link datastax-enterprise:opscenter')

    if localbuilds:
        launch_command.append('opscenter')
    else:
        launch_command.append('datastaxdemos/opscenter')

    if not debug:
        if cluster_ip:
            launch_command.append(cluster_ip)

    launch_command = ' '.join(launch_command)

    if debug:
        print 'Manually run:'
        print launch_command
    else:
        response = run(launch_command)

        if response.stderr:
            logger.error(response)
        else:
            Response = namedtuple('Response', 'container_id ip')
            container_id = response.stdout
            ip = run('docker exec %s hostname -i' % container_id).stdout
            return Response(container_id, ip)


def configure_agents(container_id, stomp_address):
    run('docker exec %s '
        'sed -i '
        '-e "s|stomp_interface:.*|stomp_interface: %s|g" '
        '/var/lib/datastax-agent/conf/address.yaml' % (
            container_id, stomp_address))
    run('docker exec %s service datastax-agent restart' % container_id)


if __name__ == '__main__':
    setup_globals()
    stop_all()
    dse_container = launch_dse(localbuilds=options.localbuilds,
                               debug=options.debug,
                               spark=True, hadoop=True,
                               sharkserver=True, hiveserver=True)

    print dse_container

    opscenter_container = launch_opscenter(localbuilds=options.localbuilds,
                                           debug=options.debug,
                                           cluster_ip=dse_container.ip)

    print opscenter_container
    configure_agents(dse_container.container_id, opscenter_container.ip)
